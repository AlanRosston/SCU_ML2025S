{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Practice 3\n",
    "\n",
    "Use the 20 Newsgroups data set available on http://qwone.com/~jason/20Newsgroups/.\n",
    "\n",
    "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of my knowledge, it was originally collected by Ken Lang, probably for his Newsweeder: Learning to filter netnews paper, though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n",
    "\n",
    "1. Read the two categories from the dataset using sklearn.datasets.load_files. You can start with ‘comp.graphics’ and ‘sci.med’.\n",
    "2. Using Scikit Learn sklearn.feature_extraction.text.CountVectorizer convert the text content into numerical feature vectors.\n",
    "3. Using Scikit Learn sklearn.feature_extraction.text.TfidfTransformer compute the TF-IDF\n",
    "   - Term Frequency (TF) = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "   - Inverse Document Frequency (IDF) = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in. The IDF of a rare word is high, whereas the IDF of a frequent word is likely to be low. Thus having the effect of highlighting words that are distinct.\n",
    "   - TF-IDF value is calculated as = TF × IDF\n",
    "4. Using Scikit Learn build a basic KNN classifier model for this dataset."
   ],
   "id": "c20a02c52291e3e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T13:35:46.667560Z",
     "start_time": "2025-07-02T13:35:45.971927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "# Choose comp.graphics and sci.med\n",
    "categories = ['comp.graphics', 'sci.med']\n",
    "\n",
    "train_path = './20news-bydate-train'\n",
    "test_path = './20news-bydate-test'\n",
    "\n",
    "train_data = load_files(train_path, categories=categories, encoding='latin1', decode_error='ignore')\n",
    "test_data = load_files(test_path, categories=categories, encoding='latin1', decode_error='ignore')\n",
    "\n",
    "print(f\"Train docs: {len(train_data.data)}, Test docs: {len(test_data.data)}\")"
   ],
   "id": "301d267c24939311",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train docs: 1178, Test docs: 785\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T13:35:51.847640Z",
     "start_time": "2025-07-02T13:35:51.689016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Convert text content into numerical feature vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(train_data.data)\n",
    "X_test_counts = vectorizer.transform(test_data.data)\n",
    "print(f\"Train shape: {X_train_counts.shape}, Test shape: {X_test_counts.shape}\")"
   ],
   "id": "c1fc459d780ac015",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1178, 24614), Test shape: (785, 24614)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T13:35:56.246259Z",
     "start_time": "2025-07-02T13:35:56.237694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "print(f\"Train TF-IDF shape: {X_train_tfidf.shape}, Test TF-IDF shape: {X_test_tfidf.shape}\")"
   ],
   "id": "3928f79ae890306d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TF-IDF shape: (1178, 24614), Test TF-IDF shape: (785, 24614)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T13:36:00.286762Z",
     "start_time": "2025-07-02T13:36:00.140087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_tfidf, train_data.target)\n",
    "y_pred = knn.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(test_data.target, y_pred))\n",
    "print(classification_report(test_data.target, y_pred, target_names=train_data.target_names))"
   ],
   "id": "bae6e33c1ce79fd2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9070063694267516\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "comp.graphics       0.93      0.88      0.90       389\n",
      "      sci.med       0.89      0.93      0.91       396\n",
      "\n",
      "     accuracy                           0.91       785\n",
      "    macro avg       0.91      0.91      0.91       785\n",
      " weighted avg       0.91      0.91      0.91       785\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
